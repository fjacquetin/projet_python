{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## La version de python utilisée est 3.12.7\n",
    "\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import lxml as lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import io as io\n",
    "import math\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import json\n",
    "from pandasgui import show\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from folium.plugins import HeatMap\n",
    "import nbconvert\n",
    "\n",
    "from script import process_data\n",
    "from script import geolocaliser\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Création du fichier des transactions cotières"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On télécharge la base Demande de valeurs foncières (DVF) qui référence, pour l'année 2023, l'ensemble des mutations à titre onéreux (en majeure partie géolocalisées) : https://files.data.gouv.fr/geo-dvf/latest/csv/2023/full.csv.gz\n",
    "\n",
    "Les informations sont issues de la Base nationale des données patrimoniales, alimentées par le système d'information de la DGFip et couvrent la France métropolitaine à l'exception des départements du Bas-Rhin, du Haut-Rhin et de Moselle.\n",
    "\n",
    "La base recense des actes (id_mutation), qui comportent un ou plusieurs mutations distinctes, repérées par le numéro de disposition (numero_disposition).\n",
    "\n",
    "Les observations de la base, appelées \"lignes de restitution\", concernent les différents locaux d'une mutation (Appartement, Maison, Dépendance, Local Industriel), ventilées selon autant de natures de culture présentes dans l'immeuble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\190562132.py:18: DtypeWarning: Columns (8,10,12,14,17,18,20,22,26,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"full.csv\",encoding=\"utf-8\")\n"
     ]
    }
   ],
   "source": [
    "# url = \"https://static.data.gouv.fr/resources/demandes-de-valeurs-foncieres/20241008-071041/valeursfoncieres-2023.txt.zip\"\n",
    "\n",
    "url = \"https://files.data.gouv.fr/geo-dvf/latest/csv/2023/full.csv.gz\"\n",
    "# Envoyer une requête HTTP pour obtenir le fichier CSV\n",
    "\n",
    "downloaded_file = \"full.csv.gz\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(downloaded_file, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# Décompresser le fichier\n",
    "with gzip.open(downloaded_file, 'rb') as f_in:\n",
    "    with open(\"full.csv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Charger le fichier CSV dans un DataFrame\n",
    "df = pd.read_csv(\"full.csv\",encoding=\"utf-8\")\n",
    "\n",
    "# Optionnel : supprimer le fichier compressé après décompression\n",
    "os.remove(\"full.csv.gz\")\n",
    "os.remove(\"full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3799407\n",
      "1356412\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "print(df['id_mutation'].unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Première analyse : afficher toutes les variables, leurs types et le pourcentage des transactions non renseignées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premiers filtres\n",
    "1) Suppression de toutes les colonnes non utilisées\n",
    "2) Suppression de toutes les transactions non renseignées\n",
    "3) Reconstitution des adresses complètes\n",
    "\n",
    "4) Retenir toutes les transactions qui correspondent à des ventes\n",
    "5) Géolocaliser toutes les transactions qui ne le sont pas dans la base originelle\n",
    "6) Renseigner les départements et régions\n",
    "\n",
    "7) On ne garde que les biens particuliers (appartements et maisons)\n",
    "8) Vérifier que la surface est renseignée\n",
    "9) Filtre sur les seuls appartements\n",
    "10) Exclure les biens dans les DROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Première opération : alléger le dataset\n",
    "\n",
    "colonnes_a_supprimer = ['adresse_suffixe',\n",
    "                        'code_nature_culture',\n",
    "                        'ancien_code_commune',\n",
    "                        'ancien_nom_commune',\n",
    "                        'ancien_id_parcelle',\n",
    "                        'numero_volume',\n",
    "                        'code_nature_culture_speciale',\n",
    "                        'nature_culture_speciale',\n",
    "                        'lot1_numero',\n",
    "                        'lot2_numero',\n",
    "                        'lot3_numero',\n",
    "                        'lot4_numero',\n",
    "                        'lot5_numero',\n",
    "                        'lot1_surface_carrez',\n",
    "                        'lot2_surface_carrez',\n",
    "                        'lot3_surface_carrez',\n",
    "                        'lot4_surface_carrez',\n",
    "                        'lot5_surface_carrez'\n",
    "                        ]\n",
    "\n",
    "df.drop(columns=colonnes_a_supprimer, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convertir les colonnes spécifiques en chaînes de caractères\n",
    "# colonnes_a_convertir = ['adresse_code_voie', 'code_commune', 'code_departement']\n",
    "# df_concatene = process_data.convertir_en_str(df_concatene, colonnes_a_convertir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URL de base pour les fichiers DVF\n",
    "# base_url = \"https://files.data.gouv.fr/geo-dvf/latest/csv/{year}/full.csv.gz\"\n",
    "\n",
    "# # Liste des années\n",
    "# annees = range(2019, 2024)\n",
    "\n",
    "# # Colonnes à supprimer\n",
    "# colonnes_a_supprimer = [\n",
    "#     'id_mutation', 'adresse_suffixe', 'numero_disposition',\n",
    "#     'code_nature_culture', 'ancien_code_commune', 'ancien_nom_commune',\n",
    "#     'ancien_id_parcelle', 'numero_volume', 'code_nature_culture_speciale',\n",
    "#     'nature_culture_speciale', 'lot1_numero', 'lot2_numero', 'lot3_numero',\n",
    "#     'lot4_numero', 'lot5_numero', 'lot1_surface_carrez', 'lot2_surface_carrez',\n",
    "#     'lot3_surface_carrez', 'lot4_surface_carrez', 'lot5_surface_carrez'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe le fichier des communes cotières et on ne retient que les transactions liées à ces communes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectuer un left join entre df_final et df_communes\n",
    "df_cotieres = pd.read_csv('data/communes_cotieres.csv',sep=\";\")\n",
    "df_cotieres = df_cotieres.rename(columns={'latitude': 'latitude_centre'})\n",
    "df_cotieres = df_cotieres.rename(columns={'longitude': 'longitude_centre'})\n",
    "\n",
    "liste_cotieres = df_cotieres['nom'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['nom_commune'].isin(liste_cotieres)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286003\n",
      "123784\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "print(df['id_mutation'].unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290\n"
     ]
    }
   ],
   "source": [
    "#Filtre 1 : On supprime tous les montants non renseignés\n",
    "print(df['valeur_fonciere'].isna().sum())\n",
    "df = df.dropna(subset=['valeur_fonciere'])\n",
    "df = df[df['valeur_fonciere']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtre 2 : On ne conserve que les ventes (hors vente du neuf)\n",
    "# df = df[df['nature_mutation']=='Vente']\n",
    "df = df[df['nature_mutation'].str.contains('Vente', na=False)]\n",
    "df = df.drop(columns=['nature_mutation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "object\n",
      "float64\n",
      "object\n",
      "62753     02316\n",
      "73247     02316\n",
      "73248     02316\n",
      "75464     02316\n",
      "135339    06159\n",
      "Name: code_commune, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3ème opération : conversion des adresses en string\n",
    "\n",
    "print(df['adresse_numero'].dtype)\n",
    "print(df['adresse_nom_voie'].dtype)\n",
    "print(df['code_postal'].dtype)\n",
    "print(df['nom_commune'].dtype)\n",
    "\n",
    "colonnes_a_nettoyer = ['adresse_numero', 'code_postal']\n",
    "df = process_data.nettoyer_colonnes(df, colonnes_a_nettoyer)\n",
    "# Convertir la colonne 'code_commune' en type string\n",
    "df['code_commune'] = df['code_commune'].astype('string')\n",
    "\n",
    "# Ajouter un '0' au début si la chaîne a 4 caractères\n",
    "df['code_commune'] = [x.zfill(5) if len(x) == 4 else x for x in df['code_commune']]\n",
    "\n",
    "# Vérifier les résultats\n",
    "print(df['code_commune'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  abreviation type_voie_complet\n",
      "0         RUE               Rue\n",
      "1          AV            Avenue\n",
      "2         RTE             Route\n",
      "3         CHE            Chemin\n",
      "4          BD         Boulevard\n"
     ]
    }
   ],
   "source": [
    "# 4ème opération (si besoin de géolocalisation) : Compléter par le type de voie\n",
    "\n",
    "voie = pd.read_csv(\"data/voie.csv\",sep=\";\",encoding=\"utf-8\")\n",
    "print(voie.head())\n",
    "\n",
    "# Liste des abréviations de types de voie\n",
    "abbreviations = voie['abreviation'].tolist()\n",
    "\n",
    "# Appliquer la fonction à la colonne 'adresse_nom_voie'\n",
    "result = [process_data.check_abbreviation(adresse,abbreviations) for adresse in df['adresse_nom_voie']]\n",
    "\n",
    "# Décomposer les résultats dans les colonnes 'type_voie' et 'nom_voie'\n",
    "df['type_voie'] = [x[0] for x in result]\n",
    "df['nom_voie'] = [x[1] for x in result]\n",
    "\n",
    "df = df.merge(voie,left_on=['type_voie'],right_on=['abreviation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                       9 Rue DE LA PLAINE 2600 Fleury\n",
      "1                       9 Rue DE LA PLAINE 2600 Fleury\n",
      "2     6 Avenue MARECHAL FOCH 6230 Villefranche-sur-Mer\n",
      "3    17 Avenue FRANCOIS DE MONLEON 6190 Roquebrune-...\n",
      "4    17 Avenue FRANCOIS DE MONLEON 6190 Roquebrune-...\n",
      "Name: Adresse, dtype: string\n"
     ]
    }
   ],
   "source": [
    "## 5ème : Réécriture de l'adresse\n",
    "\n",
    "df['Adresse'] = df['adresse_numero'] + ' ' + df['type_voie_complet'] + ' ' + df['nom_voie'] + ' ' + df['code_postal'] + ' ' + df['nom_commune']\n",
    "\n",
    "print(df['Adresse'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n",
      "  departmentCode           departmentName  regionCode  \\\n",
      "0              1                      Ain        84.0   \n",
      "1              2                    Aisne        32.0   \n",
      "2              3                   Allier        84.0   \n",
      "3              4  Alpes-de-Haute-Provence        93.0   \n",
      "4              5             Hautes-Alpes        93.0   \n",
      "\n",
      "                   regionName  \n",
      "0        Auvergne-Rhône-Alpes  \n",
      "1             Hauts-de-France  \n",
      "2        Auvergne-Rhône-Alpes  \n",
      "3  Provence-Alpes-Côte d'Azur  \n",
      "4  Provence-Alpes-Côte d'Azur  \n",
      "Type de la colonne 'departmentCode': object\n",
      "Modalités de 'departmentCode':\n",
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '21' '22' '23' '24' '25' '26' '27' '28' '29' '2A' '2B'\n",
      " '30' '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43'\n",
      " '44' '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57'\n",
      " '58' '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71'\n",
      " '72' '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85'\n",
      " '86' '87' '88' '89' '90' '91' '92' '93' '94' '95' '971' '972' '973' '974'\n",
      " '976' '987' '988']\n",
      "[2 6 8 11 13 14 17 19 21 22 24 25 27 28 29 '29' '2A' '2B' 30 32 33 34 35\n",
      " 38 40 42 44 45 50 51 52 54 56 58 59 60 62 64 65 66 69 76 77 80 83 85 88\n",
      " 89 972 974]\n"
     ]
    }
   ],
   "source": [
    "# Charger les données des codes de région\n",
    "region = pd.read_csv('data/code_region.csv', sep=';')\n",
    "print(region['departmentCode'].dtype)\n",
    "print(df['code_departement'].dtype)\n",
    "\n",
    "# Afficher les premières lignes pour vérifier\n",
    "print(region.head())\n",
    "\n",
    "# Vérifier le type de la colonne 'departmentCode'\n",
    "print(f\"Type de la colonne 'departmentCode': {region['departmentCode'].dtype}\")\n",
    "\n",
    "# Vérifier les modalités (valeurs uniques) de 'departmentCode'\n",
    "print(\"Modalités de 'departmentCode':\")\n",
    "print(region['departmentCode'].unique())\n",
    "print(df['code_departement'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '21' '22' '23' '24' '25' '26' '27' '28' '29' '2A' '2B'\n",
      " '30' '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43'\n",
      " '44' '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57'\n",
      " '58' '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71'\n",
      " '72' '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85'\n",
      " '86' '87' '88' '89' '90' '91' '92' '93' '94' '95' '971' '972' '973' '974'\n",
      " '976' '987' '988']\n",
      "['2' '6' '8' '11' '13' '14' '17' '19' '21' '22' '24' '25' '27' '28' '29'\n",
      " '2A' '2B' '30' '32' '33' '34' '35' '38' '40' '42' '44' '45' '50' '51'\n",
      " '52' '54' '56' '58' '59' '60' '62' '64' '65' '66' '69' '76' '77' '80'\n",
      " '83' '85' '88' '89' '972' '974']\n"
     ]
    }
   ],
   "source": [
    "region['departmentCode'] = region['departmentCode'].astype(str)\n",
    "df['code_departement'] = df['code_departement'].astype(str)\n",
    "\n",
    "print(region['departmentCode'].unique())\n",
    "print(df['code_departement'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Nombre de transactions  Part du total (%)\n",
      "regionName                                                           \n",
      "Auvergne-Rhône-Alpes                            99           0.043931\n",
      "Bourgogne-Franche-Comté                         68           0.030175\n",
      "Bretagne                                     37557          16.665853\n",
      "Centre-Val de Loire                             24           0.010650\n",
      "Corse                                         4507           1.999973\n",
      "Grand Est                                      119           0.052806\n",
      "Hauts-de-France                              14516           6.441450\n",
      "La Réunion                                    1050           0.465936\n",
      "Martinique                                      50           0.022187\n",
      "Normandie                                    18787           8.336698\n",
      "Nouvelle-Aquitaine                           24238          10.755570\n",
      "Occitanie                                    19522           8.662853\n",
      "Pays de la Loire                             13701           6.079795\n",
      "Provence-Alpes-Côte d'Azur                   91035          40.396622\n",
      "Île-de-France                                   80           0.035500\n"
     ]
    }
   ],
   "source": [
    "#Fusionner df_geolocalisees avec les codes de région\n",
    "df = df.merge(region, left_on=[\"code_departement\"], right_on=['departmentCode'], how='left')\n",
    "\n",
    "df['regionCode'] = df['regionCode'].astype('Int64')  # Utilise Int64 pour gérer les valeurs manquantes\n",
    "#Compter le nombre de transactions par région\n",
    "\n",
    "transactions_par_region = df.groupby('regionName').size()\n",
    "\n",
    "# Calculer le total des transactions\n",
    "total_transactions = transactions_par_region.sum()\n",
    "\n",
    "# Calculer la part de chaque région dans le total des transactions\n",
    "part_region = (transactions_par_region / total_transactions) * 100\n",
    "\n",
    "# Créer un DataFrame avec les résultats\n",
    "tableau_regions = pd.DataFrame({\n",
    "    'Nombre de transactions': transactions_par_region,\n",
    "    'Part du total (%)': part_region\n",
    "})\n",
    "\n",
    "# Afficher le tableau des régions\n",
    "print(tableau_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type_local\n",
      "Dépendance                                  96656\n",
      "Appartement                                 59213\n",
      "Maison                                      34699\n",
      "Local industriel. commercial ou assimilé     8423\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['type_local'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calcul de la base des prix au mètre carré\n",
    "\n",
    "Notre objectif est d'évaluer le prix au mètre carré des biens en distinguant les maisons et les appartements\n",
    "Une mutation peut contenir plusieurs types de locaux\n",
    "\n",
    "On conserve chaque couple mutation/disposition qui contient une nature de culture sols ou vide. On suppose que ces mutations concernent des biens à visée d'habitation. Pour ces mutations, on regarde si elles contiennent :\n",
    "- un local de type 'Maison' :  la transaction est référencée comme une maison\n",
    "- un local de type 'Appartement' (mais sans 'Maison') : la transaction est référencée comme un appartement\n",
    "- aucun local de type 'Maison' ou 'Appartement' : la transaction n'est pas retenue\n",
    "\n",
    "La valeur foncière étant dupliquée, seule sa première occurence est retenue. Le nombre de pièces retenu est le plus grand de toutes les lignes de restitution. Les surfaces réelles sont additionnées par mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   valeur_fonciere  surface_reelle_bati  surface_terrain  nombre_locaux  \\\n",
      "0          35000.0                  8.0              0.0              1   \n",
      "1         175000.0                 54.0           1401.0              3   \n",
      "2          85000.0                 38.0              0.0              2   \n",
      "3         194000.0                 52.0             15.0              5   \n",
      "4          96000.0                 55.0              0.0              1   \n",
      "\n",
      "   maison_present  appart_present  nature_culture_sols_ou_vide  \\\n",
      "0           False           False                         True   \n",
      "1            True           False                         True   \n",
      "2           False            True                         True   \n",
      "3           False            True                         True   \n",
      "4           False            True                         True   \n",
      "\n",
      "   nombre_pieces_principales   id_mutation date_mutation  ...   latitude  \\\n",
      "0                    35000.0  2023-1000010    2023-06-08  ...  49.920737   \n",
      "1                   175000.0  2023-1000028    2023-06-14  ...  49.917051   \n",
      "2                    85000.0  2023-1000113    2023-06-23  ...  49.921548   \n",
      "3                   194000.0  2023-1000150    2023-06-26  ...  49.926711   \n",
      "4                    96000.0  2023-1000158    2023-06-23  ...  49.928098   \n",
      "\n",
      "  type_voie       nom_voie abreviation type_voie_complet  \\\n",
      "0       RUE         THIERS         RUE               Rue   \n",
      "1       CHE        DU GOLF         CHE            Chemin   \n",
      "2       RUE  DE BLAINVILLE         RUE               Rue   \n",
      "3       RUE      VAUQUELIN         RUE               Rue   \n",
      "4       RUE       GUERRIER         RUE               Rue   \n",
      "\n",
      "                             Adresse departmentCode  departmentName  \\\n",
      "0         16 Rue THIERS 76200 Dieppe             76  Seine-Maritime   \n",
      "1     17 Chemin DU GOLF 76200 Dieppe             76  Seine-Maritime   \n",
      "2  29 Rue DE BLAINVILLE 76200 Dieppe             76  Seine-Maritime   \n",
      "3       9 Rue VAUQUELIN 76200 Dieppe             76  Seine-Maritime   \n",
      "4       48 Rue GUERRIER 76200 Dieppe             76  Seine-Maritime   \n",
      "\n",
      "  regionCode  regionName  \n",
      "0         28   Normandie  \n",
      "1         28   Normandie  \n",
      "2         28   Normandie  \n",
      "3         28   Normandie  \n",
      "4         28   Normandie  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\4046070118.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_mai = df.groupby('id_mutation').apply(process_group)\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour traiter chaque groupe\n",
    "def process_group(group):\n",
    "    # Vérifier si 'Maison' est présent dans type_local\n",
    "    maison_present = 'Maison' in group['type_local'].values\n",
    "    appart_present = 'Appartement' in group['type_local'].values and 'Maison' not in group['type_local'].values\n",
    "    \n",
    "    # Vérifier si 'nature_culture' contient autre chose que 'sols' ou est vide\n",
    "    nature_culture_sols_ou_vide = group['nature_culture'].isna().any() or group['nature_culture'].str.contains('sols', na=False).any()\n",
    "    \n",
    "    # Remplacer NaN par 0 pour la somme des surfaces et de la valeur foncière\n",
    "    surface_reelle_bati = group['surface_reelle_bati'].fillna(0).sum()\n",
    "    surface_terrain = group['surface_terrain'].fillna(0).sum()\n",
    "    valeur_fonciere = group['valeur_fonciere'].iloc[0]\n",
    "    nombre_pieces_principales = group['valeur_fonciere'].max()\n",
    "    \n",
    "    # Rassembler les résultats\n",
    "    result = {\n",
    "        'valeur_fonciere': valeur_fonciere,\n",
    "        'surface_reelle_bati': surface_reelle_bati,\n",
    "        'surface_terrain': surface_terrain,\n",
    "        'nombre_locaux': len(group),\n",
    "        'maison_present': maison_present,\n",
    "        'appart_present': appart_present,\n",
    "        'nature_culture_sols_ou_vide': nature_culture_sols_ou_vide,\n",
    "        'nombre_pieces_principales': nombre_pieces_principales\n",
    "    }\n",
    "    \n",
    "    # Conserver toutes les autres colonnes (première valeur par défaut)\n",
    "    for col in group.columns:\n",
    "        if col not in result:\n",
    "            result[col] = group[col].iloc[0]\n",
    "    \n",
    "    return pd.Series(result)\n",
    "\n",
    "# Appliquer le traitement\n",
    "df_mai = df.groupby('id_mutation').apply(process_group)\n",
    "\n",
    "# Utiliser `reset_index()` sans insérer `id_mutation` à nouveau\n",
    "df_mai.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Vérifier le résultat\n",
    "print(df_mai.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_mai[df_mai['nature_culture_sols_ou_vide']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['appart_present'] == True) | (df['maison_present'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_type_local(row):\n",
    "    if row['maison_present']:\n",
    "        return 'Maison'\n",
    "    elif row['appart_present']:\n",
    "        return 'Appartement'\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df['type_local'] = df.apply(assign_type_local, axis=1)\n",
    "df = df.drop(columns=['appart_present','maison_present','nature_culture'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie que toutes les transactions concernent des surfaces non nulles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['surface_reelle_bati'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prix_m2'] = df['valeur_fonciere'] / df['surface_reelle_bati']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "communes_riches = df.groupby('nom_commune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_cotieres, how='left', left_on='nom_commune', right_on='nom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sur les 92156 transactions renseignées et géolocalisées, on ne conserve que 92156 (soit 100.0%)transactions représentant 62.0% d'appartements et 38.0% de maisons.\n"
     ]
    }
   ],
   "source": [
    "# One ne garde donc que les maisons et les appartements\n",
    "\n",
    "df = df[df['type_local'].isin(['Maison', 'Appartement'])]\n",
    "\n",
    "total_transactions = df.shape[0]\n",
    "total_transactions2 = df.shape[0]\n",
    "\n",
    "# Calcul du nombre d'appartements et de maisons dans ce sous-ensemble\n",
    "appartements = df[df['type_local'] == 'Appartement'].shape[0]\n",
    "maisons = df[df['type_local'] == 'Maison'].shape[0]\n",
    "\n",
    "# Calcul des pourcentages\n",
    "pourcentage = total_transactions2/total_transactions * 100\n",
    "pourcentage_appartements = (appartements / total_transactions2) * 100\n",
    "pourcentage_maisons = (maisons / total_transactions2) * 100\n",
    "\n",
    "# Générer la phrase\n",
    "phrase = f\"Sur les {total_transactions} transactions renseignées et géolocalisées, on ne conserve que {total_transactions2} (soit {pourcentage:.1f}%)\" \\\n",
    "         f\"transactions représentant {pourcentage_appartements:.1f}% d'appartements et {pourcentage_maisons:.1f}% de maisons.\"\n",
    "\n",
    "# Afficher la phrase\n",
    "print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de doublons : 11695\n",
      "Nombre de lignes après suppression des doublons : 80152\n"
     ]
    }
   ],
   "source": [
    "# Une partie des transactions sont en doublon\n",
    "\n",
    "# Compter les doublons en fonction des colonnes spécifiées\n",
    "nb_doublons = df.duplicated(subset=['id_mutation']).sum()\n",
    "print(f\"Nombre de doublons : {nb_doublons}\")\n",
    "\n",
    "# Supprimer les doublons en gardant la première occurrence\n",
    "df = df.drop_duplicates(subset=['adresse_nom_voie', 'code_postal', 'surface_reelle_bati', 'valeur_fonciere'], keep='first')\n",
    "\n",
    "# Vérifier le nouveau nombre de lignes\n",
    "print(f\"Nombre de lignes après suppression des doublons : {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne totale des prix en fonction de la surface : 3452.00 €"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\1610620510.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group['valeur_fonciere'].sum() / group['surface_reelle_bati'].sum())\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la moyenne des prix totale en fonction de la surface\n",
    "prix_moyen_m2 = df['valeur_fonciere'].sum() / df['surface_reelle_bati'].sum()\n",
    "print(f\"Moyenne totale des prix en fonction de la surface : {prix_moyen_m2:.2f} €\")\n",
    "\n",
    "# Groupement par région et type_local pour calculer les prix moyens\n",
    "prix_moyen = (\n",
    "    df.groupby(['regionName', 'type_local'])\n",
    "    .apply(lambda group: group['valeur_fonciere'].sum() / group['surface_reelle_bati'].sum())\n",
    "    .reset_index(name='prix_moyen')\n",
    ")\n",
    "\n",
    "# Conversion en tableau croisé dynamique (pivot table)\n",
    "tableau_prix_moyen = prix_moyen.pivot_table(\n",
    "    index='regionName',\n",
    "    columns='type_local',\n",
    "    values='prix_moyen',\n",
    "    aggfunc='mean'  # Non nécessaire ici car chaque cellule est déjà agrégée\n",
    ")\n",
    "\n",
    "# Remplir les valeurs manquantes par 0 ou autre\n",
    "tableau_prix_moyen = tableau_prix_moyen.fillna(0)\n",
    "\n",
    "# Calcul du nombre de transactions par région\n",
    "nb_transactions_region = df.groupby('regionName').size()\n",
    "\n",
    "# Calcul du pourcentage d'appartements et de maisons par région\n",
    "nb_transactions_appart = df[df['type_local'] == \"Appartement\"].groupby('regionName').size()\n",
    "nb_transactions_maison = df[df['type_local'] == \"Maison\"].groupby('regionName').size()\n",
    "\n",
    "# Calculer le pourcentage pour chaque type par région\n",
    "pourcentage_appart = (nb_transactions_appart / nb_transactions_region * 100).fillna(0)\n",
    "pourcentage_maison = (nb_transactions_maison / nb_transactions_region * 100).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    regionName\n",
      "0                Île-de-France\n",
      "1          Centre-Val de Loire\n",
      "2      Bourgogne-Franche-Comté\n",
      "3                    Normandie\n",
      "4              Hauts-de-France\n",
      "5                    Grand Est\n",
      "6             Pays de la Loire\n",
      "7                     Bretagne\n",
      "8           Nouvelle-Aquitaine\n",
      "9                   Guadeloupe\n",
      "10                  Martinique\n",
      "11                      Guyane\n",
      "12                  La Réunion\n",
      "13                     Mayotte\n",
      "14                   Occitanie\n",
      "15        Auvergne-Rhône-Alpes\n",
      "16  Provence-Alpes-Côte d'Azur\n",
      "17                       Corse\n",
      "                    regionName\n",
      "0   Provence-Alpes-Côte d'Azur\n",
      "1                     Bretagne\n",
      "2           Nouvelle-Aquitaine\n",
      "3                    Occitanie\n",
      "4                    Normandie\n",
      "5              Hauts-de-France\n",
      "6             Pays de la Loire\n",
      "7                        Corse\n",
      "8                   La Réunion\n",
      "9         Auvergne-Rhône-Alpes\n",
      "10                   Grand Est\n",
      "11               Île-de-France\n",
      "12     Bourgogne-Franche-Comté\n",
      "13                  Martinique\n",
      "14         Centre-Val de Loire\n"
     ]
    }
   ],
   "source": [
    "# URL du fichier GeoJSON des régions de France\n",
    "url_region = \"https://france-geojson.gregoiredavid.fr/repo/regions.geojson\"\n",
    "\n",
    "# Récupérer le fichier GeoJSON des régions de France\n",
    "response = requests.get(url_region)\n",
    "regions_geojson = response.json()\n",
    "\n",
    "# Créer une liste pour stocker les noms des régions\n",
    "region_names = []\n",
    "\n",
    "# Extraire les noms des régions depuis les propriétés de chaque feature\n",
    "for feature in regions_geojson['features']:\n",
    "    region_name = feature['properties']['nom']\n",
    "    region_names.append(region_name)\n",
    "\n",
    "# Créer un DataFrame avec les noms des régions\n",
    "df_regions = pd.DataFrame(region_names, columns=['regionName'])\n",
    "    \n",
    "# Afficher le DataFrame\n",
    "print(df_regions)\n",
    "\n",
    " # Afficher les modalités de la colonne 'regionName'\n",
    "modalites_region = df['regionName'].value_counts().reset_index()\n",
    "\n",
    "modalites_region = pd.DataFrame(modalites_region[['regionName']])\n",
    "# Affichage\n",
    "print(modalites_region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail sur les zones cotières\n",
    "1) Import des listes des communes littorales\n",
    "2) Calcul des centroïdes des communes (afin d'orienter la visualisation sur la commune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la variable adresse_py en remplaçant les espaces par des \"+\"\n",
    "df['adresse_py'] = df['Adresse'].str.replace(' ', '+', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\1226572168.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  show(commune)\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\1226572168.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  show(commune)\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\1226572168.py:2: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\1226572168.py:2: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\1226572168.py:2: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\1226572168.py:2: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commune = pd.read_csv('data/communes_code.csv', sep=';',encoding=\"utf-8\")\n",
    "show(commune)\n",
    "# Left_join des codes_communes avec les noms des communes geoJson\n",
    "df = df.merge(commune, left_on=[\"code_commune\"], right_on=['code'], how='left')\n",
    "df = df.sort_values(by='code_commune', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sur les 80152 transactions renseignées, 0.41% ne sont pas géolocalisées.\n"
     ]
    }
   ],
   "source": [
    "# Combien de transactions ne sont pas géolocalisées ?\n",
    "\n",
    "# Nombre total de transactions renseignées (celles qui ont des valeurs dans 'latitude' et 'longitude')\n",
    "total_transactions_renseignees = df.shape[0]\n",
    "\n",
    "# Nombre de lignes où 'latitude' ou 'longitude' est manquant (sans double compte)\n",
    "non_geolocalisees = df[df['latitude'].isna() | df['longitude'].isna()].shape[0]\n",
    "\n",
    "# Calcul du pourcentage de transactions non géolocalisées\n",
    "pourcentage_non_geolocalisees = (non_geolocalisees / total_transactions_renseignees) * 100\n",
    "\n",
    "# Affichage de la phrase\n",
    "print(f\"Sur les {total_transactions_renseignees} transactions renseignées, {pourcentage_non_geolocalisees:.2f}% ne sont pas géolocalisées.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\2325933965.py:4: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\2325933965.py:4: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\2325933965.py:4: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\2325933965.py:4: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\2325933965.py:4: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\2325933965.py:4: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x218bb8e6600>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculer le nombre de transactions par commune\n",
    "nombre_transactions_cotieres = df.groupby('communeName').size().reset_index(name='Nombre de transactions')\n",
    "\n",
    "show(nombre_transactions_cotieres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = df[df['latitude'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = df_missing.drop(columns=['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = geolocaliser.geolocaliser_actifs(df_missing, 'adresse_py', 'latitude', 'longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatène les DataFrames en réinitialisant l'index\n",
    "df_final = pd.concat([df, df_missing], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transactions = df_final.drop(columns=['departmentCode','communeName','code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transactions.to_csv(\"data/base.csv\",sep=\";\",index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = base_transactions[base_transactions['type_local']=='Maison']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\3307407764.py:1: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\3307407764.py:1: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\3307407764.py:1: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\3307407764.py:1: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\3307407764.py:1: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_13580\\3307407764.py:1: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x218bb8e4680>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
